{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "'''\n",
    "\n",
    "Get toxic score for the text from Hatescan API.\n",
    "The API supports english language text.\n",
    "'''\n",
    "def hatescan_toxic_classifier(text):\n",
    "    # adding \\ infront of \" in the text\n",
    "    text = text.replace('\"', '\\\\\"')\n",
    "\n",
    "    payload = '{\"text\": \"' + text +'\"}'\n",
    "\n",
    "    json_payload =json.loads(payload, strict=False)\n",
    "\n",
    "    headers={\"Content-Type\": \"application/json; charset=utf-8\"}\n",
    "\n",
    "    # Hatescan API url\n",
    "    api_hatescan_url = 'https://hatescan.dsv.su.se/predict'\n",
    "\n",
    "    # sending post request to the API\n",
    "    hatescan_response = requests.post(api_hatescan_url, headers=headers, json=json_payload)\n",
    "\n",
    "    # return toxic probability\n",
    "    return hatescan_response.json()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # TODO\n",
    "    '''\n",
    "    1) write code to read the text file with data and store the results (from the file) in a list. Each post is an entry in the list. \n",
    "    2) loop through the list to get a post and send it to the function get_toxic_probability \n",
    "    3) The function get_toxic_probability returns the probability that a post (text) is toxic\n",
    "    4) save the probability score of the post (text) together with the text in suitable format (e.g excel, tsv)\n",
    "    '''\n",
    "\n",
    "    _path = 'stormfront_500.txt'\n",
    "    # Initialize an empty list to store the file contents\n",
    "     \n",
    "    predict, post, toxic_flag = [],[],[]\n",
    "\n",
    "    # Open the file in read mode\n",
    "    try:\n",
    "        with open(_path, 'r', encoding=\"utf-8\") as file:\n",
    "        # Read each line of the file and append it to the list\n",
    "            for line in file:\n",
    "                toxic_score = hatescan_toxic_classifier(line) \n",
    "\n",
    "                print(toxic_score, line)\n",
    "                # Assiging an flag of \"1\" to a Post if it fall above the threshold, and \"0\" to Post if it fall below the thresh hold\n",
    "                if toxic_score >= 70:\n",
    "                    _flag = 1\n",
    "                else:\n",
    "                    _flag = 0\n",
    "\n",
    "                predict = predict + [list(toxic_score.values())[0]]\n",
    "                post = post + [line]\n",
    "                toxic_flag = toxic_flag + [_flag]\n",
    "\n",
    "        #Creating a dictionary out of the 2 lists for predicted score and Post\n",
    "        content = { \"Prediction\":predict, \"Toxic ?\":toxic_flag, \"Post\":post}\n",
    "        print(content)\n",
    "\n",
    "        # Create a DataFrame from the dictionary\n",
    "        df = pd.DataFrame(content)\n",
    "\n",
    "        # Specify the Excel file name\n",
    "        excel_file = 'prediction_500_class.xlsx'\n",
    "\n",
    "        # Write the DataFrame to an Excel file\n",
    "        df.to_excel(excel_file, index=False)\n",
    "\n",
    "        print(f'Data written to {excel_file}')\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found at path: {_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
